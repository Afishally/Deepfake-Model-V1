{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-24.12.23-py2.py3-none-any.whl.metadata (876 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (1.69.0)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m139.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.12.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m152.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m234.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m149.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m171.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m169.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, tensorflow-io-gcs-filesystem, optree, opt-einsum, ml-dtypes, h5py, google-pasta, gast, astunparse, tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.15.1\n",
      "    Uninstalling tensorboard-2.15.1:\n",
      "      Successfully uninstalled tensorboard-2.15.1\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-24.12.23 gast-0.6.0 google-pasta-0.2.0 h5py-3.12.1 keras-3.8.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.13.1 tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.1.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn numpy pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from seaborn) (2.1.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from seaborn) (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "\n",
    "# import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import plotly.graph_objs as go\n",
    "# from plotly.offline import iplot\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced folders: 31701 images in 'real', 31701 images in 'fake'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the paths to the folders\n",
    "real_folder = \"Deepfake-Data-V1/real\"\n",
    "fake_folder = \"Deepfake-Data-V1/fake\"\n",
    "\n",
    "# Get all files in each folder\n",
    "real_files = os.listdir(real_folder)\n",
    "fake_files = os.listdir(fake_folder)\n",
    "\n",
    "# Calculate the minimum number of images\n",
    "min_count = min(len(real_files), len(fake_files))\n",
    "\n",
    "# Function to balance a folder by downsampling\n",
    "def balance_folder(source_folder, file_list, target_count):\n",
    "    # Shuffle the file list for randomness\n",
    "    random.shuffle(file_list)\n",
    "    # Select only the required number of files\n",
    "    selected_files = file_list[:target_count]\n",
    "    # Remove excess files\n",
    "    for file_name in file_list[target_count:]:\n",
    "        os.remove(os.path.join(source_folder, file_name))\n",
    "    return selected_files\n",
    "\n",
    "# Downsample both folders to the minimum count\n",
    "balanced_real = balance_folder(real_folder, real_files, min_count)\n",
    "balanced_fake = balance_folder(fake_folder, fake_files, min_count)\n",
    "\n",
    "print(f\"Balanced folders: {len(balanced_real)} images in 'real', {len(balanced_fake)} images in 'fake'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split complete: 22190 real train, 4755 real val, 4756 real test\n",
      "22190 fake train, 4755 fake val, 4756 fake test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the paths to the folders (after balancing)\n",
    "real_folder = \"Deepfake-Data-V1/real\"\n",
    "fake_folder = \"Deepfake-Data-V1/fake\"\n",
    "\n",
    "# Define the parent output directory for splits\n",
    "parent_output_dir = \"training_data\"\n",
    "train_dir = os.path.join(parent_output_dir, \"train\")\n",
    "test_dir = os.path.join(parent_output_dir, \"test\")\n",
    "val_dir = os.path.join(parent_output_dir, \"val\")\n",
    "\n",
    "# Create the output directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Create subdirectories for real and fake classes inside train, test, and val\n",
    "os.makedirs(os.path.join(train_dir, \"real\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, \"fake\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(test_dir, \"real\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(test_dir, \"fake\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(val_dir, \"real\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(val_dir, \"fake\"), exist_ok=True)\n",
    "\n",
    "# Split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Function to split data\n",
    "def split_data(file_list, train_ratio, val_ratio, test_ratio):\n",
    "    random.shuffle(file_list)\n",
    "    total_count = len(file_list)\n",
    "    \n",
    "    train_count = int(total_count * train_ratio)\n",
    "    val_count = int(total_count * val_ratio)\n",
    "    \n",
    "    train_files = file_list[:train_count]\n",
    "    val_files = file_list[train_count:train_count+val_count]\n",
    "    test_files = file_list[train_count+val_count:]\n",
    "    \n",
    "    return train_files, val_files, test_files\n",
    "\n",
    "# Get balanced files (after balancing)\n",
    "real_files = balanced_real\n",
    "fake_files = balanced_fake\n",
    "\n",
    "# Split the files for both real and fake classes\n",
    "real_train, real_val, real_test = split_data(real_files, train_ratio, val_ratio, test_ratio)\n",
    "fake_train, fake_val, fake_test = split_data(fake_files, train_ratio, val_ratio, test_ratio)\n",
    "\n",
    "# Function to copy files to the appropriate directory\n",
    "def copy_files(file_list, source_folder, target_folder):\n",
    "    for file_name in file_list:\n",
    "        src_path = os.path.join(source_folder, file_name)\n",
    "        dest_path = os.path.join(target_folder, file_name)\n",
    "        shutil.copy(src_path, dest_path)\n",
    "\n",
    "# Copy files to the respective directories\n",
    "copy_files(real_train, real_folder, os.path.join(train_dir, \"real\"))\n",
    "copy_files(fake_train, fake_folder, os.path.join(train_dir, \"fake\"))\n",
    "copy_files(real_val, real_folder, os.path.join(val_dir, \"real\"))\n",
    "copy_files(fake_val, fake_folder, os.path.join(val_dir, \"fake\"))\n",
    "copy_files(real_test, real_folder, os.path.join(test_dir, \"real\"))\n",
    "copy_files(fake_test, fake_folder, os.path.join(test_dir, \"fake\"))\n",
    "\n",
    "print(f\"Data split complete: {len(real_train)} real train, {len(real_val)} real val, {len(real_test)} real test\")\n",
    "print(f\"{len(fake_train)} fake train, {len(fake_val)} fake val, {len(fake_test)} fake test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is using the GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if TensorFlow is using the GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(\"TensorFlow is using the GPU\")\n",
    "else:\n",
    "    print(\"TensorFlow is not using the GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1737126552.380184    3040 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from functools import partial\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define your Conv2D layer with partial\n",
    "DefaultConv2D = partial(layers.Conv2D, kernel_size=3, padding=\"same\",\n",
    "                        activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "\n",
    "# Model Definition\n",
    "model = models.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[224, 224, 3]),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(units=128, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(units=64, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(units=1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Function to load and preprocess the images (without resizing)\n",
    "def load_and_preprocess_images(image_paths):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image_path in image_paths:\n",
    "        img_array = img_to_array(load_img(image_path))  # Load the image and convert to array\n",
    "        img_array = img_array / 255.0  # Normalize pixel values to [0, 1]\n",
    "        images.append(img_array)\n",
    "        label = 0 if \"real\" in image_path else 1  # Label: 0 for real, 1 for fake\n",
    "        labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Paths to the train, validation, and test folders\n",
    "train_dir = \"training_data/train\"\n",
    "val_dir = \"training_data/val\"\n",
    "test_dir = \"training_data/test\"\n",
    "\n",
    "# Get all image paths\n",
    "train_real_paths = [os.path.join(train_dir, \"real\", fname) for fname in os.listdir(os.path.join(train_dir, \"real\"))]\n",
    "train_fake_paths = [os.path.join(train_dir, \"fake\", fname) for fname in os.listdir(os.path.join(train_dir, \"fake\"))]\n",
    "val_real_paths = [os.path.join(val_dir, \"real\", fname) for fname in os.listdir(os.path.join(val_dir, \"real\"))]\n",
    "val_fake_paths = [os.path.join(val_dir, \"fake\", fname) for fname in os.listdir(os.path.join(val_dir, \"fake\"))]\n",
    "test_real_paths = [os.path.join(test_dir, \"real\", fname) for fname in os.listdir(os.path.join(test_dir, \"real\"))]\n",
    "test_fake_paths = [os.path.join(test_dir, \"fake\", fname) for fname in os.listdir(os.path.join(test_dir, \"fake\"))]\n",
    "\n",
    "# Load and preprocess images\n",
    "train_images, train_labels = load_and_preprocess_images(train_real_paths + train_fake_paths)\n",
    "val_images, val_labels = load_and_preprocess_images(val_real_paths + val_fake_paths)\n",
    "test_images, test_labels = load_and_preprocess_images(test_real_paths + test_fake_paths)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "# tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_images, train_labels,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_data=(val_images, val_labels)\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"CNN_from_scratch.h5\")  # Saves the model in HDF5 format\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(64, kernel_size=7, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\", input_shape=[224, 224, 3]),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(128, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(32, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a model.summary here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44380 files belonging to 2 classes.\n",
      "Found 9510 files belonging to 2 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1737127908.314302   27818 service.cc:148] XLA service 0x7fe6840023d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1737127908.315044   27818 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-01-17 15:31:48.597346: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1737127909.097859   27818 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-01-17 15:31:51.468806: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[64,128,112,112]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,64,112,112]{3,2,1,0}, f32[128,64,3,3]{3,2,1,0}, f32[128]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
      "2025-01-17 15:31:53.215482: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[64,128,112,112]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,128,112,112]{3,2,1,0}, f32[128,128,3,3]{3,2,1,0}, f32[128]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
      "I0000 00:00:1737127930.430558   27818 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m693/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.5996 - loss: 0.7660"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 15:35:14.372694: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[28,128,112,112]{3,2,1,0}, u8[0]{0}) custom-call(f32[28,64,112,112]{3,2,1,0}, f32[128,64,3,3]{3,2,1,0}, f32[128]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
      "2025-01-17 15:35:15.631152: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[28,128,112,112]{3,2,1,0}, u8[0]{0}) custom-call(f32[28,128,112,112]{3,2,1,0}, f32[128,128,3,3]{3,2,1,0}, f32[128]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - accuracy: 0.5996 - loss: 0.7659"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 15:35:27.498718: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[64,128,112,112]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,64,112,112]{3,2,1,0}, f32[128,64,3,3]{3,2,1,0}, f32[128]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
      "2025-01-17 15:35:28.995326: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[64,128,112,112]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,128,112,112]{3,2,1,0}, f32[128,128,3,3]{3,2,1,0}, f32[128]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
      "2025-01-17 15:35:44.294524: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[38,128,112,112]{3,2,1,0}, u8[0]{0}) custom-call(f32[38,64,112,112]{3,2,1,0}, f32[128,64,3,3]{3,2,1,0}, f32[128]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
      "2025-01-17 15:35:45.571892: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k25=0} for conv (f32[38,128,112,112]{3,2,1,0}, u8[0]{0}) custom-call(f32[38,128,112,112]{3,2,1,0}, f32[128,128,3,3]{3,2,1,0}, f32[128]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 314ms/step - accuracy: 0.5997 - loss: 0.7658 - val_accuracy: 0.6774 - val_loss: 0.6136\n",
      "Epoch 2/10\n",
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 280ms/step - accuracy: 0.6760 - loss: 0.6166 - val_accuracy: 0.6976 - val_loss: 0.5895\n",
      "Epoch 3/10\n",
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 280ms/step - accuracy: 0.7016 - loss: 0.5865 - val_accuracy: 0.6820 - val_loss: 0.5971\n",
      "Epoch 4/10\n",
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 280ms/step - accuracy: 0.7174 - loss: 0.5672 - val_accuracy: 0.7101 - val_loss: 0.5715\n",
      "Epoch 5/10\n",
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 280ms/step - accuracy: 0.7333 - loss: 0.5427 - val_accuracy: 0.7141 - val_loss: 0.5757\n",
      "Epoch 6/10\n",
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 280ms/step - accuracy: 0.7535 - loss: 0.5095 - val_accuracy: 0.6968 - val_loss: 0.6134\n",
      "Epoch 7/10\n",
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 280ms/step - accuracy: 0.7824 - loss: 0.4637 - val_accuracy: 0.6921 - val_loss: 0.6783\n",
      "Epoch 8/10\n",
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 280ms/step - accuracy: 0.7975 - loss: 0.4363 - val_accuracy: 0.6525 - val_loss: 0.7677\n",
      "Epoch 9/10\n",
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 280ms/step - accuracy: 0.8399 - loss: 0.3600 - val_accuracy: 0.6655 - val_loss: 0.8086\n",
      "Epoch 10/10\n",
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 280ms/step - accuracy: 0.8728 - loss: 0.2932 - val_accuracy: 0.7139 - val_loss: 0.7209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define directory paths\n",
    "train_dir = \"training_data/train\"\n",
    "val_dir = \"training_data/val\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=(224, 224),  # Resize images to 224x224\n",
    "    batch_size=64,  # Load in batches\n",
    "    label_mode='binary'  # Binary classification: \"real\" vs. \"fake\"\n",
    ")\n",
    "\n",
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    label_mode='binary'\n",
    ")\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "normalization_layer = tf.keras.layers.Rescaling(1.0 / 255)\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_dataset = val_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# Prefetch for improved performance\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Stop when validation loss stops improving\n",
    "    patience=5,  # Number of epochs with no improvement before stopping\n",
    "    restore_best_weights=True  # Restore the best model weights\n",
    ")\n",
    "\n",
    "# Logging to a CSV file\n",
    "csv_logger = CSVLogger(\"training_log_CNN.csv\", append=False)  # Overwrite previous logs\n",
    "\n",
    "# Train the model with callbacks\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping, csv_logger]  # Add callbacks here\n",
    ")\n",
    "\n",
    "model.save(\"CNN_from_scratch.h5\")  # Saves the model in HDF5 format\n",
    "\n",
    "# Load the logged data\n",
    "log_data = pd.read_csv(\"training_log_CNN.csv\")\n",
    "\n",
    "# Plot accuracy and loss curves\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "ax[0].plot(log_data[\"epoch\"], log_data[\"accuracy\"], label=\"Train Accuracy\")\n",
    "ax[0].plot(log_data[\"epoch\"], log_data[\"val_accuracy\"], label=\"Val Accuracy\")\n",
    "ax[0].set_title(\"Accuracy Curve\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "ax[0].legend()\n",
    "\n",
    "# Loss plot\n",
    "ax[1].plot(log_data[\"epoch\"], log_data[\"loss\"], label=\"Train Loss\")\n",
    "ax[1].plot(log_data[\"epoch\"], log_data[\"val_loss\"], label=\"Val Loss\")\n",
    "ax[1].set_title(\"Loss Curve\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try transfer learning next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44380 files belonging to 2 classes.\n",
      "Found 9510 files belonging to 2 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 14:46:24.462802: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1737211584.473014    7988 service.cc:148] XLA service 0x7f646403c050 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1737211584.473830    7988 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-01-18 14:46:25.105003: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1737211586.001686    7988 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1737211597.140569    7988 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.6609 - loss: 0.6302"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 14:49:10.130426: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 277ms/step - accuracy: 0.6609 - loss: 0.6302 - val_accuracy: 0.6977 - val_loss: 0.5830\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 14:50:46.212369: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.6944 - loss: 0.5881"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 14:53:12.432790: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 270ms/step - accuracy: 0.6944 - loss: 0.5881 - val_accuracy: 0.7132 - val_loss: 0.5663\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 14:53:53.397590: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.7028 - loss: 0.5756"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 14:56:20.246529: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 257ms/step - accuracy: 0.7028 - loss: 0.5756 - val_accuracy: 0.7096 - val_loss: 0.5610\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 14:56:51.723146: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.7099 - loss: 0.5615"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 14:59:19.112119: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 258ms/step - accuracy: 0.7099 - loss: 0.5615 - val_accuracy: 0.7132 - val_loss: 0.5551\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 14:59:50.568504: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 258ms/step - accuracy: 0.7184 - loss: 0.5520 - val_accuracy: 0.7192 - val_loss: 0.5512\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 15:02:49.272640: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.7252 - loss: 0.5402"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 15:05:16.691490: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 258ms/step - accuracy: 0.7252 - loss: 0.5402 - val_accuracy: 0.7134 - val_loss: 0.5531\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 15:05:48.142662: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.7291 - loss: 0.5320"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 15:08:15.555838: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 258ms/step - accuracy: 0.7291 - loss: 0.5320 - val_accuracy: 0.7270 - val_loss: 0.5353\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 15:08:46.997225: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.7362 - loss: 0.5250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 15:11:14.417194: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 258ms/step - accuracy: 0.7362 - loss: 0.5250 - val_accuracy: 0.7244 - val_loss: 0.5356\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 15:11:45.876566: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.7393 - loss: 0.5150"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 15:14:13.258915: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 258ms/step - accuracy: 0.7393 - loss: 0.5150 - val_accuracy: 0.7322 - val_loss: 0.5275\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 15:14:44.706594: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.7492 - loss: 0.5041"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 15:17:12.110596: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 67109120 bytes after encountering the first element of size 67109120 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 258ms/step - accuracy: 0.7492 - loss: 0.5041 - val_accuracy: 0.7259 - val_loss: 0.5332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define directory paths\n",
    "train_dir = \"training_data/train\"\n",
    "val_dir = \"training_data/val\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=(224, 224),  # Resize images to 224x224\n",
    "    batch_size=64,  # Load in batches\n",
    "    label_mode='binary'  # Binary classification: \"real\" vs. \"fake\"\n",
    ")\n",
    "\n",
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    label_mode='binary'\n",
    ")\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "normalization_layer = tf.keras.layers.Rescaling(1.0 / 255)\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_dataset = val_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# Prefetch for improved performance\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Xception model\n",
    "base_model = tf.keras.applications.Xception(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Stop when validation loss stops improving\n",
    "    patience=3,  # Wait for 3 epochs before stopping\n",
    "    restore_best_weights=True  # Keep the best model weights\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\"training_log_X.csv\", append=False)  # Overwrite previous logs\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping, csv_logger]  # Add callbacks here\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"xception_TL_model.h5\")\n",
    "\n",
    "# Load the logged data\n",
    "log_data = pd.read_csv(\"training_log_X.csv\")\n",
    "\n",
    "# Plot accuracy and loss curves\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "ax[0].plot(log_data[\"epoch\"], log_data[\"accuracy\"], label=\"Train Accuracy\")\n",
    "ax[0].plot(log_data[\"epoch\"], log_data[\"val_accuracy\"], label=\"Val Accuracy\")\n",
    "ax[0].set_title(\"Accuracy Curve\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "ax[0].legend()\n",
    "\n",
    "# Loss plot\n",
    "ax[1].plot(log_data[\"epoch\"], log_data[\"loss\"], label=\"Train Loss\")\n",
    "ax[1].plot(log_data[\"epoch\"], log_data[\"val_loss\"], label=\"Val Loss\")\n",
    "ax[1].set_title(\"Loss Curve\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ xception (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ xception (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m20,861,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m262,272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,648,685</span> (82.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,648,685\u001b[0m (82.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262,401</span> (1.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m262,401\u001b[0m (1.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> (79.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m20,861,480\u001b[0m (79.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">524,804</span> (2.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m524,804\u001b[0m (2.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(base_model.output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21 02:53:30.707448: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-21 02:53:31.204036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737428011.364384    2596 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737428011.424123    2596 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-21 02:53:31.791194: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-21 02:53:40.107747: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpkg_2k_b9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpkg_2k_b9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpkg_2k_b9'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer_2')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140406728196512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728207952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728209888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728203376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728205664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728208832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728207072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728207248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728211120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728344496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728210240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728211472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728346432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728348896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728349776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728346960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728347312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728349424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728348720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728352240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728350304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728350480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728353120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728356640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728358048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728354528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728355760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728358400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728359104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728444384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728442272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728350832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728442976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728346080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728354704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728442624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728445792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728447552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728444032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728447200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728451600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728453008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728449488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728450720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728450368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728453536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728455648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728457056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728453888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728456528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728454592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728449312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728562416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728557840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728559776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728558896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728559424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728561536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728564176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728558720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728562592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728558192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728564880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728566992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728568400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728565232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728567872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728561184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728568576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728566816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728568752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728569984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728570688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728566288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728565584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728622848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728623904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728623200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728626016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728625488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728627600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728629712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728627776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728627952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728630592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728624608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728631296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728634112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728635520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728632352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728634992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728631120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728636928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728705472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728706000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728633408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728704944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728633584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728633760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728707408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728706880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728707232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728707936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728708288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728710400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728713216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728711280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728711456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728714096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728709696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728714800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728717616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728719024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728715856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728718496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728719904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728708464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728819984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728821920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728821040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728821568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728824560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728717440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728823680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728823152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728823504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728824208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728822448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728826672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728829488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728830896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728827728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728830368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728822624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728831600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728829312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728830192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728822976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728831776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728833184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728827376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728886400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728888864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728889744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728886928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728888336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728889392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728892208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728890272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728890448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728893088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728887456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728894672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728896784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728898192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728895024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728897664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728895728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728899600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729001264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729000912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728896080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728999856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728894496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728896256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729002672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729004432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406728999152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729003200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729001792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729006544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729008656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729010064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729006896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729009536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729001968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729010240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729013056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729014464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729011296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729013936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729012000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729000736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729066096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729065040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729064864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729067680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729066624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729069264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729071376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729069440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729065568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729072256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729067152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729072960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729075776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729077184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729074016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729076656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729080528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729077888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729075600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729074720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729075072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710306064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729079120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406729075248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710307472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710309232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710305360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710308000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710308352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710310464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710309760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710313280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710311344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710311520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710314160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710317680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710319088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710315568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710316800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710319440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710320144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710389040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710388688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710318208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710387632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710318736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710313808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710390448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710392208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710386928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710390976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406740221296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406730853360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710396080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140406710393440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1737428027.524946    2596 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1737428027.525206    2596 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-01-21 02:53:47.527470: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpkg_2k_b9\n",
      "2025-01-21 02:53:47.537113: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-01-21 02:53:47.537151: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpkg_2k_b9\n",
      "I0000 00:00:1737428027.635068    2596 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "2025-01-21 02:53:47.656085: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-01-21 02:53:48.411537: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpkg_2k_b9\n",
      "2025-01-21 02:53:48.543626: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 1016167 microseconds.\n",
      "2025-01-21 02:53:48.745553: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully saved as TensorFlow Lite at xception_TL_model.tflite\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('xception_TL_model.h5')\n",
    "# Save the model weights\n",
    "# model.save_weights('xception_model.weights.h5')\n",
    "\n",
    "# Convert the model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model to a file\n",
    "tflite_model_path = 'xception_TL_model.tflite'\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"Model successfully saved as TensorFlow Lite at {tflite_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 678ms/step\n",
      "Predictions: [[0.49020678]]\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "model = tf.keras.models.load_model('xception_TL_model.h5')\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Open the image using Pillow\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Resize the image to match the input size of the model (224x224 for Xception)\n",
    "    image = image.resize((224, 224))\n",
    "    \n",
    "    # Convert image to numpy array and normalize\n",
    "    image_np = np.array(image) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "    # Add batch dimension (model expects a batch of images)\n",
    "    image_batch = np.expand_dims(image_np, axis=0)\n",
    "    \n",
    "    return image_batch\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = 'training_data/val/real/ymprxvokzn.jpg'  # Provide your image path here\n",
    "processed_image = preprocess_image(image_path)\n",
    "\n",
    "# Run inference\n",
    "predictions = model.predict(processed_image)\n",
    "\n",
    "# Print the predictions (depending on the model, this may output probabilities or class labels)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation function of the last layer: <function sigmoid at 0x7fbf586c24d0>\n",
      "Last layer weights: [[-0.1285923 ]\n",
      " [ 0.18582182]\n",
      " [ 0.02886584]\n",
      " [ 0.23456906]\n",
      " [-0.1395561 ]\n",
      " [ 0.0186427 ]\n",
      " [ 0.19600216]\n",
      " [-0.08143987]\n",
      " [ 0.11895234]\n",
      " [ 0.18107797]\n",
      " [-0.21679403]\n",
      " [-0.24925193]\n",
      " [ 0.15858735]\n",
      " [ 0.07501129]\n",
      " [-0.2408136 ]\n",
      " [ 0.20246278]\n",
      " [ 0.10204855]\n",
      " [ 0.22343624]\n",
      " [ 0.13057308]\n",
      " [ 0.1687005 ]\n",
      " [ 0.02541616]\n",
      " [ 0.23907204]\n",
      " [ 0.20263854]\n",
      " [ 0.16465536]\n",
      " [-0.22030453]\n",
      " [-0.06494074]\n",
      " [ 0.15042076]\n",
      " [ 0.16515432]\n",
      " [-0.15356612]\n",
      " [ 0.18708235]\n",
      " [ 0.20329712]\n",
      " [-0.14540787]\n",
      " [ 0.11765339]\n",
      " [-0.22018161]\n",
      " [ 0.15332218]\n",
      " [ 0.02960869]\n",
      " [-0.20265944]\n",
      " [-0.1483899 ]\n",
      " [-0.16090497]\n",
      " [ 0.27037996]\n",
      " [ 0.18354677]\n",
      " [ 0.18915085]\n",
      " [ 0.20433314]\n",
      " [-0.21465255]\n",
      " [-0.11221179]\n",
      " [-0.10400102]\n",
      " [ 0.08108465]\n",
      " [ 0.0780916 ]\n",
      " [ 0.08076163]\n",
      " [ 0.20119524]\n",
      " [ 0.18695444]\n",
      " [-0.00676524]\n",
      " [-0.13765693]\n",
      " [ 0.21280092]\n",
      " [-0.20046711]\n",
      " [-0.09169381]\n",
      " [-0.12215361]\n",
      " [ 0.32298064]\n",
      " [ 0.21998046]\n",
      " [-0.15850672]\n",
      " [ 0.10148028]\n",
      " [-0.15940024]\n",
      " [ 0.1850466 ]\n",
      " [-0.2503006 ]\n",
      " [-0.215653  ]\n",
      " [-0.16321169]\n",
      " [ 0.13412726]\n",
      " [ 0.14439091]\n",
      " [-0.10229512]\n",
      " [-0.19637522]\n",
      " [-0.19775085]\n",
      " [ 0.20755753]\n",
      " [ 0.14605723]\n",
      " [ 0.13487507]\n",
      " [ 0.10423127]\n",
      " [ 0.2242956 ]\n",
      " [ 0.16085543]\n",
      " [-0.15475139]\n",
      " [-0.18174402]\n",
      " [ 0.23846094]\n",
      " [ 0.06719754]\n",
      " [-0.02231845]\n",
      " [ 0.1407746 ]\n",
      " [ 0.06998584]\n",
      " [ 0.13848065]\n",
      " [-0.10452284]\n",
      " [-0.13238144]\n",
      " [-0.22059867]\n",
      " [-0.01130841]\n",
      " [ 0.17293458]\n",
      " [-0.27292243]\n",
      " [-0.21029267]\n",
      " [-0.02299194]\n",
      " [ 0.1301136 ]\n",
      " [ 0.20088325]\n",
      " [ 0.27315503]\n",
      " [ 0.15669735]\n",
      " [-0.18049426]\n",
      " [-0.11445938]\n",
      " [ 0.14565113]\n",
      " [ 0.27984372]\n",
      " [-0.16154695]\n",
      " [-0.22445229]\n",
      " [ 0.15819246]\n",
      " [-0.16476318]\n",
      " [ 0.18573275]\n",
      " [ 0.16992687]\n",
      " [-0.17170003]\n",
      " [ 0.13465272]\n",
      " [ 0.03699379]\n",
      " [ 0.17157564]\n",
      " [-0.12188473]\n",
      " [ 0.20402612]\n",
      " [-0.07103627]\n",
      " [ 0.13390109]\n",
      " [-0.25273487]\n",
      " [ 0.11444728]\n",
      " [ 0.07142472]\n",
      " [-0.02926034]\n",
      " [-0.18066251]\n",
      " [ 0.24454854]\n",
      " [-0.08335749]\n",
      " [-0.2613169 ]\n",
      " [-0.19592083]\n",
      " [ 0.17204364]\n",
      " [ 0.07023825]\n",
      " [ 0.21175595]\n",
      " [ 0.18328205]]\n",
      "Last layer biases: [-0.13000292]\n"
     ]
    }
   ],
   "source": [
    "# Get the last layer of the model\n",
    "last_layer = model.layers[-1]\n",
    "\n",
    "# Check the activation function\n",
    "print(f\"Activation function of the last layer: {last_layer.activation}\")\n",
    "\n",
    "# Check the weights and biases\n",
    "weights, biases = last_layer.get_weights()\n",
    "print(f\"Last layer weights: {weights}\")\n",
    "print(f\"Last layer biases: {biases}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44380 files belonging to 2 classes.\n",
      "Found 9510 files belonging to 2 classes.\n",
      "['fake', 'real']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Define directory paths\n",
    "train_dir = \"training_data/train\"\n",
    "val_dir = \"training_data/val\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=(224, 224),  # Resize images to 224x224\n",
    "    batch_size=64,  # Load in batches\n",
    "    label_mode='binary'  # Binary classification: \"real\" vs. \"fake\"\n",
    ")\n",
    "\n",
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    label_mode='binary'\n",
    ")\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "normalization_layer = tf.keras.layers.Rescaling(1.0 / 255)\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_dataset = val_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# Prefetch for improved performance\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Access class names from directory structure\n",
    "class_names = os.listdir(train_dir)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
